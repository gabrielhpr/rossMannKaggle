{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 0.0. Preparing process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "hide_input": true
   },
   "source": [
    "## 0.1. IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:41:43.253000Z",
     "start_time": "2021-02-08T19:41:43.232741Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import datetime\n",
    "import inflection\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from boruta                  import BorutaPy\n",
    "from scipy                   import stats as ss\n",
    "from tabulate                import tabulate\n",
    "from IPython.display         import Image\n",
    "from sklearn.metrics         import mean_absolute_error, mean_squared_error\n",
    "from sklearn.ensemble        import RandomForestRegressor\n",
    "from sklearn.linear_model    import LinearRegression, Lasso\n",
    "from sklearn.preprocessing   import RobustScaler, MinMaxScaler, LabelEncoder\n",
    "\n",
    "# import jtplot submodule from jupyterthemes\n",
    "#from jupyterthemes import jtplot\n",
    "# currently installed theme will be used to\n",
    "# set plot style if no arguments provided\n",
    "#jtplot.style()\n",
    "\n",
    "###--------- PANDAS - EXIBIR TODAS COLUNAS ----###\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "###--------- ESTILIZAÇÃO DO NOTEBOOK ---------###\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# retira a margem do notebook\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# font do texto markdown\n",
    "display(HTML(\"<style>h1 { font-size:23px !important; }</style>\"))\n",
    "display(HTML(\"<style>h2 { font-size:20px !important; }</style>\"))\n",
    "display(HTML(\"<style>h3 { font-size:17px !important; }</style>\"))\n",
    "display(HTML(\"<style>h4 { font-size:16px !important; }</style>\"))\n",
    "display(HTML(\"<style>p { font-size:16px !important; }</style>\"))\n",
    "\n",
    "# tamanho da fonte da tabela\n",
    "display(HTML(\"<style>th { font-size:15px !important; }</style>\"))\n",
    "display(HTML(\"<style>td { font-size:15px !important; }</style>\"))\n",
    "\n",
    "# font do codigo \n",
    "display(HTML(\"<style>span { font-size:16px !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 0.2. Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "   jt -t monokai -T -N -f roboto -fs 12 -cellw 93% -lineh 185 -ofs 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 0.3. Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Função que calcula o Crammer V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:24:35.759894Z",
     "start_time": "2021-02-08T19:24:35.170974Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def cramer_v( var1, var2 ):\n",
    "    cm = pd.crosstab( var1, var2 ).to_numpy()\n",
    "    n = cm.sum()\n",
    "    r, k = cm.shape\n",
    "    \n",
    "    chi2 = ss.chi2_contingency( cm )[0]\n",
    "    chi2corr = max(0, (chi2/n) - (k-1)*(r-1)/(n-1) )\n",
    "    \n",
    "    kcorr = k - (k-1)**2/(n-1)\n",
    "    rcorr = r - (r-1)**2/(n-1)\n",
    "    \n",
    "    return np.sqrt( chi2corr / min( kcorr-1, rcorr-1 ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Measuring Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:24:35.945979Z",
     "start_time": "2021-02-08T19:24:35.761634Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y, yhat):\n",
    "    return np.mean( np.abs( (y - yhat) / y ) )\n",
    "\n",
    "def ml_error( model_name, y, yhat ):\n",
    "    mae = mean_absolute_error( y, yhat )\n",
    "    mape = mean_absolute_percentage_error( y, yhat )\n",
    "    rmse = np.sqrt( mean_squared_error( y, yhat) )\n",
    "    \n",
    "    return pd.DataFrame( { 'Model Name': model_name,\n",
    "                           'MAE': mae,\n",
    "                           'MAPE': mape,\n",
    "                           'RMSE': rmse }, index=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Cross validation - Realiza o cross validation e devolve o erro médio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:24:36.091852Z",
     "start_time": "2021-02-08T19:24:35.947724Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def cross_val_err( x_train, kfold, model_name, model, verbose=False ):\n",
    "    \n",
    "    mae_list = []\n",
    "    mape_list = []\n",
    "    rmse_list = []\n",
    "    x_training = x_train[ cols_selected_boruta ]\n",
    "\n",
    "    for k in reversed( range(1, kfold+1) ):\n",
    "        \n",
    "        if verbose:\n",
    "            print('\\nKfold number: ' + str(k))\n",
    "        # start and end date\n",
    "        validation_start_date = x_training['date'].max() - datetime.timedelta( days= 7 * 6 * k )\n",
    "        validation_end_date = x_training['date'].max() - datetime.timedelta( days= 7 * 6 * (k-1) )\n",
    "\n",
    "        # filtering dataset\n",
    "        training = x_training[ (x_training['date'] < validation_start_date) ]\n",
    "        validation = x_training[ (x_training['date'] >= validation_start_date) & (x_training['date'] <= validation_end_date) ]\n",
    "\n",
    "        # training dataset\n",
    "        xtraining = training.drop( ['date', 'sales'], axis=1 )\n",
    "        ytraining = training['sales']\n",
    "\n",
    "        # validation dataset\n",
    "        xvalidation = validation.drop( ['date', 'sales'], axis=1 )\n",
    "        yvalidation = validation['sales']\n",
    "\n",
    "        # model\n",
    "        md = model.fit( xtraining, ytraining )\n",
    "\n",
    "        # prediction\n",
    "        yhat_md = md.predict( xvalidation )\n",
    "\n",
    "        # performance\n",
    "        resp_error = ml_error(model_name, np.expm1( yvalidation ), np.expm1( yhat_md ) )\n",
    "\n",
    "        # adding the error to the list\n",
    "        mae_list.append( resp_error['MAE'].values )\n",
    "        mape_list.append( resp_error['MAPE'].values )\n",
    "        rmse_list.append( resp_error['RMSE'].values )\n",
    "        \n",
    "    return pd.DataFrame({ 'Model name': model_name,\n",
    "                          'MAE CV' : np.round( np.mean( np.array( mae_list ) ), 2).astype(str) + ' +/- ' + np.round( np.std( np.array( mae_list ) ) ).astype(str),\n",
    "                          'MAPE CV': np.round( np.mean( np.array( mape_list ) ), 2).astype(str) + ' +/- ' + np.round( np.std( np.array( mape_list ) ) ).astype(str),\n",
    "                          'RMSE CV': np.round( np.mean( np.array( rmse_list ) ), 2).astype(str) + ' +/- ' + np.round( np.std( np.array( rmse_list ) ) ).astype(str)\n",
    "                         }, index=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 0.4. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:24:37.524691Z",
     "start_time": "2021-02-08T19:24:36.106128Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# False makes an eager reading, instead of lazy reading\n",
    "df_sales_raw = pd.read_csv('data/train.csv', low_memory=False) \n",
    "df_store_raw = pd.read_csv('data/store.csv', low_memory=False) \n",
    "\n",
    "# merge the sales with the store, because store contains adittional informationo about\n",
    "# the stores\n",
    "df_raw = pd.merge(df_sales_raw, df_store_raw, how='left', on='Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:24:37.978496Z",
     "start_time": "2021-02-08T19:24:37.529426Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_sales_raw.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 1.0. Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 1.0.1 Saving the past data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:24:38.169320Z",
     "start_time": "2021-02-08T19:24:37.984994Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# By creating a copy of the dataframe in each section you can speed up the process of\n",
    "# recovering the original data if you overwrite it unintencionally\n",
    "df1 = df_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 1.1. Rename Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Generally the columns come from a database, so the columns' name are given by\n",
    "backend engineers and not always these names are easy to remember, then its important to \n",
    "rename the columns to speed up the data analysis project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:24:38.177398Z",
     "start_time": "2021-02-08T19:24:38.171574Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cols_old = df1.columns\n",
    "cols_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:24:38.276892Z",
     "start_time": "2021-02-08T19:24:38.179793Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "snakecase = lambda x : inflection.underscore(x)\n",
    "\n",
    "# map applies a lambda function to something\n",
    "cols_new = list( map( snakecase, cols_old) )\n",
    "print(cols_new)\n",
    "\n",
    "# rename\n",
    "df1.columns = cols_new\n",
    "print(df1.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 1.2. Data Dimensions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:24:38.369800Z",
     "start_time": "2021-02-08T19:24:38.293186Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print( 'Number of rows: {}'.format( df1.shape[0] ) )\n",
    "print( 'Number of columns: {}'.format( df1.shape[1] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 1.3. Data Types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:24:38.652261Z",
     "start_time": "2021-02-08T19:24:38.376405Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# In pandas, object refers to strings, and pandas assumes that everything that is not\n",
    "# an int or float is an object\n",
    "\n",
    "# Converting date from object to date\n",
    "df1['date'] = pd.to_datetime( df1['date'] )\n",
    "print(df1.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 1.4. Check NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T12:08:01.106955Z",
     "start_time": "2021-01-11T12:08:01.103955Z"
    },
    "hidden": true
   },
   "source": [
    " NA refers to Not Aplicable or Not Available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### There are 3 ways to treat NA\n",
    "> Remove the NA \n",
    "\n",
    ">> Advantages: Its quick and easy to do\n",
    "\n",
    ">> Disadvantages: You loose data\n",
    "              \n",
    " > Change the NA by the mean or the median\n",
    " \n",
    " > Understand the NA by business view "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:24:38.852063Z",
     "start_time": "2021-02-08T19:24:38.654275Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df1.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 1.5. Fillout NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### competition_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    " Assumptions to the NA value: the competitor is really far from the store, so it's like it doesnt exist, \n",
    " \n",
    " Assume: if we assume a really big distance, bigger than the maximum distance, it\n",
    " will be well represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:24:38.863629Z",
     "start_time": "2021-02-08T19:24:38.854109Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "competition_distance_max = df1['competition_distance'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:24:39.173807Z",
     "start_time": "2021-02-08T19:24:38.865458Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# competition_distance - the NA will be replaced by 2000 * competition_distance_max\n",
    "df1['competition_distance'] = df1['competition_distance'].apply(lambda x : (4 * competition_distance_max) if math.isnan(x) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### competition_open_since_month / year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Assumptions to the NA value: \n",
    "\n",
    "1-) The competitors store was opened before the store\n",
    "\n",
    "2-) The competitors store was opened after the store, but someone forgot to take note of it\n",
    "\n",
    "Assume: we can fill this with the date of the sell, because in the future we can use this\n",
    "information to show what happened with the sells since the appearance of the nearest competitor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:25:19.730729Z",
     "start_time": "2021-02-08T19:24:39.176217Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# axis = 1, apply function in each row\n",
    "\n",
    "# competition_open_since_month\n",
    "df1['competition_open_since_month'] = df1.apply(lambda x : x['date'].month if math.isnan( x['competition_open_since_month'] ) else x['competition_open_since_month'], axis = 1)\n",
    "\n",
    "# competition_open_since_year\n",
    "df1['competition_open_since_year'] = df1.apply(lambda x : x['date'].year if math.isnan( x['competition_open_since_year'] ) else x['competition_open_since_year'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T14:19:03.499039Z",
     "start_time": "2021-01-11T14:19:03.495563Z"
    },
    "hidden": true
   },
   "source": [
    "### promo2_since_week / year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Assumptions to the NA value: \n",
    "\n",
    "1-) The store decided to not participate of promo 2\n",
    "\n",
    "Assume: we can fill this with the date of the sell, because in the future we can use this\n",
    "information to show what happened with the sells since the store decided to not participate of promo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:25:54.939678Z",
     "start_time": "2021-02-08T19:25:19.733703Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# promo2_since_week\n",
    "df1['promo2_since_week'] = df1.apply(lambda x : x['date'].week if math.isnan(x['promo2_since_week']) else x['promo2_since_week'], axis=1)\n",
    "\n",
    "# promo2_since_year\n",
    "df1['promo2_since_year'] = df1.apply(lambda x : x['date'].year if math.isnan(x['promo2_since_year']) else x['promo2_since_year'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### promo_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:26:06.766456Z",
     "start_time": "2021-02-08T19:25:54.941558Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# promo_interval   \n",
    "month_map = { 1: 'Jan', 2: 'Feb', 3: 'Mar', 4: 'Apr', 5: 'May', 6: 'Jun', 7: 'Jul', \n",
    "            8: 'Aug', 9: 'Sep', 10: 'Out', 11: 'Nov', 12: 'Dec'}\n",
    "\n",
    "# inplace=True do the modification directly in the object instead of returning the modification\n",
    "df1['promo_interval'].fillna(0, inplace=True)\n",
    "\n",
    "df1['month_map'] = df1['date'].dt.month.map( month_map )\n",
    "\n",
    "df1['is_promo2'] = df1[['promo_interval', 'month_map']].apply(lambda x : 0 if x['promo_interval'] == 0 else 1 if x['month_map'] in x['promo_interval'].split(',') else 0, axis=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:26:06.857280Z",
     "start_time": "2021-02-08T19:26:06.767794Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df1.sample(5).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 1.6 Change Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:26:06.904811Z",
     "start_time": "2021-02-08T19:26:06.858985Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:26:07.048904Z",
     "start_time": "2021-02-08T19:26:06.906686Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df1['competition_open_since_month'] = df1['competition_open_since_month'].astype(int)\n",
    "df1['competition_open_since_year'] = df1['competition_open_since_year'].astype(int)\n",
    "df1['promo2_since_week'] = df1['promo2_since_week'].astype(int)\n",
    "df1['promo2_since_year'] = df1['promo2_since_year'].astype(int)\n",
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 1.7. Descriptive Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T19:56:00.403604Z",
     "start_time": "2021-01-11T19:56:00.400305Z"
    },
    "hidden": true
   },
   "source": [
    "### Split the data between numerical and categorial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:26:08.028815Z",
     "start_time": "2021-02-08T19:26:07.053713Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_attributes = df1.select_dtypes( include=['int64', 'float64'] )\n",
    "cat_attributes = df1.select_dtypes( exclude=['int64', 'float64', 'datetime64[ns]'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Central Tendency Metrics \n",
    "- Mean, Median \n",
    "- Can represent a set of numbers using just one number\n",
    "- When the mean and the median are almost equal, represents that there arent many \n",
    "    outliers, and we can think about a normal as a good distribution to our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Dispersive Metrics \n",
    "- Variance, Standard Deviation, Min, Max, Range, Skew, kurtosis \n",
    "- Show if the set of numbers are concentratred or sparsed\n",
    "- Skew: \n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/6/63/Skew_normal_densities.svg\" style=\"width:40%; height:40%; margin-left:10%;\"/>\n",
    "- Kurtosis: \n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/3/33/Standard_symmetric_pdfs.svg\" style=\"width:40%; height:40%; margin-left:10%; background-color: #f2f2f2\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:26:11.580007Z",
     "start_time": "2021-02-08T19:26:08.030422Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ct1 = pd.DataFrame( num_attributes.apply( np.mean ) ).T\n",
    "ct2 = pd.DataFrame( num_attributes.apply( np.median ) ).T\n",
    "\n",
    "d1 = pd.DataFrame( num_attributes.apply( np.std ) ).T\n",
    "d2 = pd.DataFrame( num_attributes.apply( min ) ).T\n",
    "d3 = pd.DataFrame( num_attributes.apply( max ) ).T\n",
    "# Calculating the range\n",
    "d4 = pd.DataFrame( num_attributes.apply( lambda x: x.max() - x.min() ) ).T\n",
    "d5 = pd.DataFrame( num_attributes.apply( lambda x: x.skew() ) ).T\n",
    "d6 = pd.DataFrame( num_attributes.apply( lambda x: x.kurtosis() ) ).T\n",
    "\n",
    "# Concatenate\n",
    "m = pd.concat( [d2, d3, d4, ct1, ct2, d1, d5, d6] ).T.reset_index()\n",
    "m.columns = ['attributes', 'min', 'max', 'range', 'mean', 'median', 'std', 'skew', 'kurtosis']\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:26:18.067818Z",
     "start_time": "2021-02-08T19:26:11.582208Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "sns.distplot( df1['competition_distance'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:26:18.228679Z",
     "start_time": "2021-02-08T19:26:18.069500Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Categorical attributes\n",
    "cat_attributes.apply(lambda x : x.unique().shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Search about boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:26:18.726746Z",
     "start_time": "2021-02-08T19:26:18.230655Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# If it was a state holiday and the sales were bigger than 0, that is the store was not\n",
    "# closed\n",
    "aux1 = df1[ (df1['state_holiday'] != '0') & (df1['sales'] > 0) ]\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.boxplot( x='state_holiday', y='sales', data=aux1)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.boxplot( x='store_type', y='sales', data=aux1)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.boxplot( x='assortment', y='sales', data=aux1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 2.0. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:26:18.780711Z",
     "start_time": "2021-02-08T19:26:18.728787Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df2 = df1.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## A motivacao de Feature Engineer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T14:32:46.303346Z",
     "start_time": "2021-01-18T14:32:46.297588Z"
    },
    "hidden": true
   },
   "source": [
    "Ter as variáveis DISPONÍVEIS para ESTUDO durante a Análise Exploratória de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Criar as variáveis durante a Análise Exploratória de Dados torna o código bagunçado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:26:19.203756Z",
     "start_time": "2021-02-08T19:26:18.782128Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Image('img/mind_map.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 2.1. Criação de hipóteses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 2.1.1. Hipótese Loja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**1.** Lojas com maior número de funcionários deveriam vender mais.\n",
    "\n",
    "**2.** Lojas com maior capacidade de estoque deveriam vender mais.\n",
    "\n",
    "**3.** Lojas com maior porte deveriam vender mais.\n",
    "\n",
    "**4.** Lojas com maior sortimento deveriam vender mais.\n",
    "\n",
    "**5.** Lojas com competidores mais próximos deveriam vender menos.\n",
    "\n",
    "**6.** Lojas com competidores a mais tempo deveriam vender mais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 2.1.2. Hipótese Produto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**1.** Lojas que investem mais em Marketing deveriam vender mais.\n",
    "\n",
    "**2.** Lojas que exibem mais os seus produtos deveriam vender mais.\n",
    "\n",
    "**3.** Lojas com produtos com preços menores deveriam vender mais.\n",
    "\n",
    "**4.** Lojas com promoções mais agressivas deveriam vender mais.\n",
    "\n",
    "**5.** Lojas com promoções ativas por mais tempo deveriam vender mais.\n",
    "\n",
    "**6.** Lojas com mais dias de promoção deveriam vender mais.\n",
    "\n",
    "**7.** Lojas com mais promoções consecutivas deveriam vender mais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 2.1.3. Hipótese Temporal (Sazonalidade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T17:56:53.449530Z",
     "start_time": "2021-01-18T17:56:53.441083Z"
    },
    "hidden": true
   },
   "source": [
    "**1.** Lojas abertas durante o feriado de Natal deveriam vender mais.\n",
    "\n",
    "**2.** Lojas deveriam vender mais ao longo dos anos.\n",
    "\n",
    "**3.** Lojas deveriam vender mais no segundo semestre do ano.\n",
    "\n",
    "**4.** Lojas deveriam vender mais depois do dia 10 de cada mês.\n",
    "\n",
    "**5.** Lojas deveriam vender menos aos finais de semana.\n",
    "\n",
    "**6.** Lojas deveriam vender menos durante as férias escolares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 2.2. Lista final de hipóteses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Escolher as hipóteses que há os dados disponíveis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T18:14:43.401866Z",
     "start_time": "2021-01-18T18:14:43.388927Z"
    },
    "hidden": true
   },
   "source": [
    "**1.** Lojas com maior sortimento deveriam vender mais.\n",
    "\n",
    "**2.** Lojas com competidores mais próximos deveriam vender menos.\n",
    "\n",
    "**3.** Lojas com competidores a mais tempo deveriam vender mais.\n",
    "\n",
    "**4.** Lojas com promoções ativas por mais tempo deveriam vender mais.\n",
    "\n",
    "**5.** Lojas com mais dias de promoção deveriam vender mais.\n",
    "\n",
    "**6.** Lojas com mais promoções consecutivas deveriam vender mais.\n",
    "\n",
    "**7.** Lojas abertas durante o feriado de Natal deveriam vender mais.\n",
    "\n",
    "**8.** Lojas deveriam vender mais ao longo dos anos.\n",
    "\n",
    "**9.** Lojas deveriam vender mais no segundo semestre do ano.\n",
    "\n",
    "**10.** Lojas deveriam vender mais depois do dia 10 de cada mês.\n",
    "\n",
    "**11.** Lojas deveriam vender menos aos finais de semana.\n",
    "\n",
    "**12.** Lojas deveriam vender menos durante as férias escolares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "hide_input": false
   },
   "source": [
    "## Derivando variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:07.471614Z",
     "start_time": "2021-02-08T19:26:19.211052Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# year\n",
    "df2['year'] = df2['date'].dt.year\n",
    "\n",
    "# month\n",
    "df2['month'] = df2['date'].dt.month\n",
    "\n",
    "# day\n",
    "df2['day'] = df2['date'].dt.day\n",
    "\n",
    "# week of year\n",
    "df2['week_of_year'] = df2['date'].dt.week\n",
    "\n",
    "# year week - changing the date format\n",
    "df2['year_week'] = df2['date'].dt.strftime('%Y-%W')\n",
    "\n",
    "\n",
    "# competition since\n",
    "df2['competition_since'] = df2.apply(lambda x: datetime.datetime(year=x['competition_open_since_year'] , month=x['competition_open_since_month'], day=1), axis=1)\n",
    "df2['competition_time_month'] = ((df2['date'] - df2['competition_since']) / 30).apply(lambda x: x.days).astype(int)\n",
    "\n",
    "# promo since\n",
    "df2['promo_since'] = df2['promo2_since_year'].astype(str) + '-' + df2['promo2_since_week'].astype(str)\n",
    "df2['promo_since'] = df2['promo_since'].apply(lambda x: datetime.datetime.strptime(x + '-1', '%Y-%W-%w') - datetime.timedelta(days=7))\n",
    "df2['promo_time_week'] = ((df2['date'] - df2['promo_since'])/7).apply(lambda x: x.days).astype(int)\n",
    "\n",
    "# assortment\n",
    "# level: a = basic, b = extra, c = extended\n",
    "df2['assortment'] = df2['assortment'].apply(lambda x: 'basic' if x == 'a' else 'extra' if x == 'b' else 'extended')\n",
    "\n",
    "# state holiday\n",
    "# a = public holiday, b = Easter holiday, c = Christmas, 0 = None\n",
    "df2['state_holiday'] = df2['state_holiday'].apply(lambda x: 'public_holiday' if x == 'a' else 'easter_holiday' if x == 'b' else 'christimas' if x == 'c' else 'regular_day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:07.502222Z",
     "start_time": "2021-02-08T19:27:07.483370Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df2.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 3.0. Filtragem de variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:07.804361Z",
     "start_time": "2021-02-08T19:27:07.504555Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df3 = df2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Primeiro fazemos a filtragem das linhas para diminuir o volume do dataset, depois realizamos a seleção das linhas num dataset já menor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 3.1. Filtragem das linhas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "'open' só queremos as que tem valor igual a 1, pois quando tem 0 a loja não abriu, esse dado não é útil no treinamento\n",
    "\n",
    "'open' != 0\n",
    "\n",
    "'sales' > 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:08.405280Z",
     "start_time": "2021-02-08T19:27:07.806155Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df3 = df3[(df3['open'] != 0) & (df3['sales'] > 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 3.2. Seleção das colunas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "'customers', não temos esse dado no momento da predição\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:08.505793Z",
     "start_time": "2021-02-08T19:27:08.406980Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cols_drop = ['customers', 'open', 'promo_interval', 'month_map']\n",
    "df3 = df3.drop(cols_drop, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 4.0. Análise Exploratória dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:08.595144Z",
     "start_time": "2021-02-08T19:27:08.508183Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df4 = df3.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 4.1. Análise Univariada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 4.1.1. Response variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:12.481622Z",
     "start_time": "2021-02-08T19:27:08.597094Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    " sns.distplot( df4['sales'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 4.1.2. Numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:16.893001Z",
     "start_time": "2021-02-08T19:27:12.485011Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#num_attributes.columns\n",
    "num_attributes.hist(figsize=(25,25), layout=(5,3), bins=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 4.1.3. Categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:16.915860Z",
     "start_time": "2021-02-08T19:27:16.895164Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cat_attributes.columns\n",
    "df4['assortment'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:27.163267Z",
     "start_time": "2021-02-08T19:27:16.921242Z"
    },
    "hidden": true,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# state_holiday\n",
    "plt.figure(figsize=(20,12))\n",
    "plt.subplot(3, 2, 1)\n",
    "state_holiday_without_reg_day = df4[ df4['state_holiday'] != 'regular_day']\n",
    "sns.countplot( state_holiday_without_reg_day['state_holiday'] )\n",
    "\n",
    "plt.subplot(3, 2, 2)\n",
    "sns.kdeplot( df4[ df4['state_holiday'] == 'public_holiday']['sales'], label='public_holiday' , shade=True )\n",
    "sns.kdeplot( df4[ df4['state_holiday'] == 'easter_holiday']['sales'], label='easter_holiday' , shade=True )\n",
    "sns.kdeplot( df4[ df4['state_holiday'] == 'christimas']['sales'], label='christimas' , shade=True )\n",
    "plt.legend(ncol=2, loc='upper right');\n",
    "\n",
    "\n",
    "# store_type\n",
    "plt.subplot(3, 2, 3)\n",
    "sns.countplot( df4['store_type'] )\n",
    "\n",
    "plt.subplot(3, 2, 4)\n",
    "sns.kdeplot( df4[ df4['store_type'] == 'a']['sales'], label='a' , shade=True )\n",
    "sns.kdeplot( df4[ df4['store_type'] == 'b']['sales'], label='b' , shade=True )\n",
    "sns.kdeplot( df4[ df4['store_type'] == 'c']['sales'], label='c' , shade=True )\n",
    "sns.kdeplot( df4[ df4['store_type'] == 'd']['sales'], label='d' , shade=True )\n",
    "plt.legend(ncol=2, loc='upper right');\n",
    "\n",
    "\n",
    "# assortment\n",
    "plt.subplot(3, 2, 5)\n",
    "sns.countplot( df4['assortment'] )\n",
    "\n",
    "plt.subplot(3, 2, 6)\n",
    "sns.kdeplot( df4[ df4['assortment'] == 'basic']['sales'], label='basic' , shade=True )\n",
    "sns.kdeplot( df4[ df4['assortment'] == 'extended']['sales'], label='extended' , shade=True )\n",
    "sns.kdeplot( df4[ df4['assortment'] == 'extra']['sales'], label='extra' , shade=True )\n",
    "plt.legend(ncol=2, loc='upper right');\n",
    "\n",
    "\n",
    "# 'state_holiday', 'store_type', 'assortment', 'promo_interval','month_map'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 4.2. Análise Bivariada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### HIP 1.  Lojas com maior sortimento deveriam vender mais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**VERDADEIRO**\n",
    "\n",
    "Foi percebido que em volume de vendas as lojas com assortment extra vende menos, mas isso\n",
    "se deve ao fato de poucas lojas possuirem tal sortimento, quando analisamos a media das\n",
    "vendas das lojas em cada tipo de sortimento, percebemos que as lojas com sortimento extra\n",
    "vendem mais do que as outras e que as possuem extendend assortment vendem mais que as do \n",
    "tipo basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:28.112913Z",
     "start_time": "2021-02-08T19:27:27.165775Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('A quantidade de lojas com cada tipo de sortimento e: \\n')\n",
    "print(df4.groupby('assortment').size())\n",
    "\n",
    "aux1 = df4[['assortment', 'sales']].groupby('assortment').mean().reset_index()\n",
    "sns.barplot(x='assortment', y='sales', data=aux1, label='opa').set_title('Media de vendas de cada tipo de sortimento');\n",
    "\n",
    "aux2 = df4[['assortment', 'year_week', 'sales']].groupby(['assortment','year_week']).mean().reset_index()\n",
    "aux2.pivot(index='year_week', columns='assortment', values='sales').plot(figsize=(9,9), ylabel='sales').set_title('Media de vendas por assortment ao longo do ano')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### <s> HIP 2. Lojas com competidores mais próximos deveriam vender menos. </s>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Inconclusivo, dificil perceber uma relação entre a distância e as vendas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.218202Z",
     "start_time": "2021-02-08T19:27:28.115028Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "aux = df4[['competition_distance', 'sales']].groupby('competition_distance').mean().reset_index()\n",
    "\n",
    "# scatter plot\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.xlim(0, 20000)\n",
    "sns.scatterplot(x='competition_distance', y='sales', data=aux).set_title('Vendas de acordo com a distancia do competidor mais proximo')\n",
    "\n",
    "\n",
    "# barplot\n",
    "bins = list( np.arange(0, 20000, 1000) )\n",
    "\n",
    "aux['competition_distance_binned'] = pd.cut( aux['competition_distance'], bins=bins)\n",
    "aux2 = aux[['competition_distance_binned','sales']].groupby('competition_distance_binned').mean().reset_index()\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.xticks( rotation='90' )\n",
    "sns.barplot(x='competition_distance_binned', y='sales', data=aux2).set_title('Vendas de acordo com a distancia do competidor mais proximo')\n",
    "\n",
    "\n",
    "# heatmap\n",
    "plt.figure(figsize=(7,7))\n",
    "x = sns.heatmap( aux.corr( method='pearson'), annot=True).set_title('correlacao entre vendas e distancia do competidor');\n",
    "bottom, top = x.get_ylim()\n",
    "x.set_ylim(bottom+0.5, top-0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### HIP 3. Lojas com competidores a mais tempo deveriam vender mais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T21:36:44.695313Z",
     "start_time": "2021-01-22T21:36:44.686857Z"
    },
    "hidden": true
   },
   "source": [
    "**VERDADEIRO** - Lojas com competidores a mais tempo vendem mais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.220407Z",
     "start_time": "2021-02-08T19:24:35.431Z"
    },
    "hidden": true,
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aux1 = df4[['competition_time_month','sales']].groupby('competition_time_month').mean().reset_index()\n",
    "aux2 = aux1[ (aux1['competition_time_month'] < 120) & (aux1['competition_time_month'] != 0) ]\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.subplot(1,3,1)\n",
    "plt.xticks(rotation=90)\n",
    "sns.barplot(x='competition_time_month', y='sales', data=aux1)\n",
    "\n",
    "\n",
    "#plt.figure(figsize=(25,7))\n",
    "plt.subplot(1,3,2)\n",
    "plt.xticks(rotation=90)\n",
    "sns.regplot(x='competition_time_month', y='sales', data=aux1)\n",
    "\n",
    "\n",
    "#plt.figure(figsize=(7,7))\n",
    "plt.subplot(1,3,3)\n",
    "x = sns.heatmap( aux1.corr( method='pearson'), annot=True).set_title('correlacao entre vendas e distancia do ');\n",
    "bottom, top = x.get_ylim()\n",
    "x.set_ylim(bottom+0.5, top-0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### HIP 4. Lojas com promoções ativas por mais tempo deveriam vender mais.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**VERDADEIRO** Todas as vendas com promo time negativo sao vendas feitas no periodo da promocao\n",
    "tradicional, sem incluir o periodo da promocao estendida que e a promo2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.222182Z",
     "start_time": "2021-02-08T19:24:35.438Z"
    },
    "hidden": true,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "aux = df4[['promo_time_week', 'sales']].groupby('promo_time_week').mean().reset_index()\n",
    "aux2 = aux[ (aux['promo_time_week'] > 0) ]\n",
    "aux3 = aux[ (aux['promo_time_week'] <= 0) ]\n",
    "\n",
    "grid = plt.GridSpec(3,3)\n",
    "\n",
    "# promo 2 ativa\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(grid[0,0])\n",
    "plt.xticks(rotation=90)\n",
    "sns.barplot(x='promo_time_week', y='sales', data=aux2).set_title('Periodo de funcionamento da promo 2')\n",
    "\n",
    "plt.subplot(grid[0,1])\n",
    "plt.xticks(rotation=90)\n",
    "sns.regplot(x='promo_time_week', y='sales', data=aux2).set_title('Periodo de funcionamento da promo 2')\n",
    "\n",
    "\n",
    "# promo 2 nao ativa\n",
    "plt.subplot(grid[1,0])\n",
    "plt.xticks(rotation=90)\n",
    "sns.barplot(x='promo_time_week', y='sales', data=aux3).set_title('Periodo de nao funcionamento da promo 2')\n",
    "\n",
    "plt.subplot(grid[1,1])\n",
    "plt.xticks(rotation=90)\n",
    "sns.regplot(x='promo_time_week', y='sales', data=aux3).set_title('Periodo de nao funcionamento da promo 2')\n",
    "\n",
    "\n",
    "# promo 2 periodo total\n",
    "plt.subplot(grid[2,0])\n",
    "plt.xticks(rotation=90)\n",
    "sns.barplot(x='promo_time_week', y='sales', data=aux).set_title('\\nPeriodo total')\n",
    "\n",
    "plt.subplot(grid[2,1])\n",
    "plt.xticks(rotation=90)\n",
    "sns.regplot(x='promo_time_week', y='sales', data=aux).set_title('\\nPeriodo total')\n",
    "\n",
    "plt.subplot(grid[:,2])\n",
    "sns.heatmap(aux.corr(method='pearson'), annot=True).set_title('\\nPeriodo total')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### HIP 5. Lojas com mais dias de promoção deveriam vender mais.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### [bom insight] HIP 6. Lojas com mais promoções consecutivas deveriam vender mais.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**FALSA** Lojas com mais promoções consecutivas vendem menos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.223840Z",
     "start_time": "2021-02-08T19:24:35.445Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df4[['promo','promo2','sales']].groupby(['promo','promo2']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.225442Z",
     "start_time": "2021-02-08T19:24:35.450Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "aux = df4[ (df4['promo'] == 1) & (df4['promo2'] == 1) ][['year_week', 'sales']].groupby('year_week').mean().reset_index()\n",
    "ax = aux.plot(figsize=(17,7))\n",
    "\n",
    "aux2 = df4[ (df4['promo'] == 1) & (df4['promo2'] == 0) ][['year_week', 'sales']].groupby('year_week').mean().reset_index()\n",
    "aux2.plot(figsize=(17,7), ax=ax)\n",
    "\n",
    "ax.legend(['tradicional + estendida', 'tradicional'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### HIP 7. Lojas abertas durante o feriado de Natal deveriam vender mais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**VERDADEIRO** No feriado de natal e de páscoa as vendas por dia são as maiores do ano - Talvez não seja um insight pois\n",
    "há uma grande do pessoal de vendas saber acerca disso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.227246Z",
     "start_time": "2021-02-08T19:24:35.456Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "aux = df4[['state_holiday', 'sales']].groupby('state_holiday').mean().reset_index()\n",
    "plt.figure(figsize=(15,7))\n",
    "sns.barplot(x='state_holiday', y='sales', data=aux).set_title('Venda média dos tipos de dias')\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "aux2 = df4[['state_holiday', 'year', 'sales']].groupby(['year','state_holiday']).mean().reset_index()\n",
    "sns.barplot(x='year', y='sales', hue='state_holiday', data=aux2).set_title('Venda média do tipo de dia ao longo dos anos')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### HIP 8. Lojas deveriam vender mais ao longo dos anos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**FALSO** As lojas vendem menos ao longo dos anos - Não forma um insight, pois o pessoal de vendas e negócios já sabem\n",
    "acerca desse fenômeno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.229144Z",
     "start_time": "2021-02-08T19:24:35.463Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "aux = df4[df4['year'] != 2015][['year', 'sales']].groupby('year').sum().reset_index()\n",
    "\n",
    "plt.figure(figsize=(20,7))\n",
    "plt.subplot(1,3,1)\n",
    "sns.barplot(x='year', y='sales', data=aux).set_title('Volume de vendas por ano')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "sns.regplot(x='year', y='sales', data=aux).set_title('Volume de vendas por ano')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "sns.heatmap(aux.corr(method='pearson'), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### HIP 9. Lojas deveriam vender mais no segundo semestre do ano.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**VERDADEIRO** As lojas vendem mais no segundo semestre do ano - Não gera um insight, pelo mesmo motivo do anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.230699Z",
     "start_time": "2021-02-08T19:24:35.469Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "aux = df4[ ['year','month', 'sales'] ].groupby(['year', 'month']).sum().reset_index()\n",
    "plt.figure(figsize=(15,7))\n",
    "sns.barplot(x='year', y='sales', hue='month', data=aux)\n",
    "\n",
    "aux2 = df4[ df4['year'] == 2013 ][ ['month', 'sales'] ].groupby('month').sum().reset_index()\n",
    "plt.figure(figsize=(15,7))\n",
    "sns.regplot(x='month', y='sales', data=aux2).set_title('Vendas mensais no ano de 2013')\n",
    "\n",
    "aux3 = df4[ df4['year'] == 2014 ][ ['month', 'sales'] ].groupby('month').sum().reset_index()\n",
    "plt.figure(figsize=(15,7))\n",
    "sns.regplot(x='month', y='sales', data=aux3).set_title('Vendas mensais no ano de 2014')\n",
    "\n",
    "aux3 = df4[ df4['year'] != 2015 ][ ['month', 'sales'] ].groupby('month').sum().reset_index()\n",
    "plt.figure(figsize=(15,7))\n",
    "sns.regplot(x='month', y='sales', data=aux3).set_title('Vendas mensais no período 2013-2014')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### HIP 10. Lojas deveriam vender mais depois do dia 10 de cada mês.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**FALSO** Separando o mês em 3 partes, os 10 primeiros dias apresenta a maior média de vendas dos 3 períodos, desconsiderando os feriados - Não gera um insight pelo mesmo motivo anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.233621Z",
     "start_time": "2021-02-08T19:24:35.476Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "aux = df4[ df4['state_holiday'] == 'regular_day' ][['day','sales']].groupby('day').mean().reset_index()\n",
    "plt.figure(figsize=(15,7))\n",
    "sns.barplot(x='day', y='sales', data=aux)\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "sns.regplot(x='day', y='sales', data=aux)\n",
    "\n",
    "aux['3_partes_do_mes'] = aux['day'].apply(lambda x: 1 if x <= 10 else 2 if ((x > 10) & (x <= 20 )) else 3)\n",
    "aux2 = aux[['3_partes_do_mes', 'sales']].groupby('3_partes_do_mes').mean().reset_index()\n",
    "plt.figure(figsize=(15,7))\n",
    "sns.barplot(x='3_partes_do_mes', y='sales', data=aux2)\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "sns.heatmap(aux2.corr(method='pearson'), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### HIP 11. Lojas deveriam vender menos aos finais de semana.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**FALSO** Das lojas que vendem ao domingo, estas apresentam a maior quantidade de vendas em relação aos demais \n",
    "dias da semana - Não gera um insight pelo mesmo motivo anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.236199Z",
     "start_time": "2021-02-08T19:24:35.484Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "aux = df4[['day_of_week', 'sales']].groupby('day_of_week').mean().reset_index()\n",
    "print(aux)\n",
    "\n",
    "print(df4['day_of_week'].value_counts())\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "sns.barplot(x='day_of_week', y='sales', data=aux)\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "sns.regplot(x='day_of_week', y='sales', data=aux)\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "sns.heatmap(aux.corr(method='pearson'), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### HIP 12. Lojas deveriam vender menos durante as férias escolares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**FALSO** Durante os dias que não há aulas escolares, as lojas vendem mais - Não gera um insight pelo mesmo motivo anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.238169Z",
     "start_time": "2021-02-08T19:24:35.490Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "aux = df4[['school_holiday','sales']].groupby('school_holiday').mean().reset_index()\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "sns.barplot(x='school_holiday', y='sales', data=aux)\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "sns.regplot(x='school_holiday', y='sales', data=aux)\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "sns.heatmap(aux.corr(method='pearson'), annot=True)\n",
    "\n",
    "aux2 = df4[['school_holiday','sales', 'month']].groupby(['school_holiday', 'month']).mean().reset_index()\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "sns.barplot(x='month', y='sales', hue='school_holiday',data=aux2)\n",
    "\n",
    "\n",
    "aux3 = df4[['school_holiday','sales', 'month']].groupby(['school_holiday', 'month']).sum().reset_index()\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "sns.barplot(x='month', y='sales', hue='school_holiday',data=aux3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### RESUMO DAS HIPÓTESES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T21:58:02.648235Z",
     "start_time": "2021-01-22T21:58:02.631467Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 4.3. Análise Multivariada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 4.3.1. Atributos numéricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.239815Z",
     "start_time": "2021-02-08T19:24:35.502Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "correlation = num_attributes.corr(method='pearson')\n",
    "plt.figure(figsize=(22,10))\n",
    "sns.heatmap( correlation, annot=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 4.3.2. Atributos categóricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.241718Z",
     "start_time": "2021-02-08T19:24:35.507Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Get all categorical data\n",
    "a = df4.select_dtypes( include='object' )\n",
    "\n",
    "# Calculate Cramer V for all combination of categorical data\n",
    "a1 = cramer_v( a['state_holiday'], a['state_holiday'] )\n",
    "a2 = cramer_v( a['state_holiday'], a['store_type'] )\n",
    "a3 = cramer_v( a['state_holiday'], a['assortment'] )\n",
    "\n",
    "a4 = cramer_v( a['store_type'], a['state_holiday'] )\n",
    "a5 = cramer_v( a['store_type'], a['store_type'] )\n",
    "a6 = cramer_v( a['store_type'], a['assortment'] )\n",
    "\n",
    "a7 = cramer_v( a['assortment'], a['state_holiday'] )\n",
    "a8 = cramer_v( a['assortment'], a['store_type'] )\n",
    "a9 = cramer_v( a['assortment'], a['assortment'] )\n",
    "\n",
    "tab = pd.DataFrame({  \n",
    "                'state_holiday': [a1,a2,a3],\n",
    "                'store_type': [a4,a5,a6],\n",
    "                'assortment': [a7,a8,a9]})\n",
    "tab.set_index(tab.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.243289Z",
     "start_time": "2021-02-08T19:24:35.510Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "sns.heatmap(tab.corr(method='pearson'), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 5.0. Preparação dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.246145Z",
     "start_time": "2021-02-08T19:24:35.517Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df5 = df4.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 5.1. Normalização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 5.2. Rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.249071Z",
     "start_time": "2021-02-08T19:24:35.523Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "aux = df5.select_dtypes(include = ['int64', 'float64'])\n",
    "aux.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.250850Z",
     "start_time": "2021-02-08T19:24:35.527Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.boxplot( df5['year'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.252805Z",
     "start_time": "2021-02-08T19:24:35.530Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rs = RobustScaler()\n",
    "mms = MinMaxScaler()\n",
    "\n",
    "#--- competition_distance ---#\n",
    "# Encontra os parâmetros (quartiles) e aplica na variável\n",
    "df5['competition_distance2'] = rs.fit_transform( df5[['competition_distance']].values )\n",
    "pickle.dump(rs, open('parameter/competition_distance2_scaler.pkl', 'wb'))\n",
    "\n",
    "#--- competition time month ---#\n",
    "df5['competition_time_month'] = rs.fit_transform( df5[['competition_time_month']].values )\n",
    "pickle.dump(rs, open('parameter/competition_time_month_scaler.pkl', 'wb'))\n",
    "\n",
    "#--- promo_time_week ---#\n",
    "# poucos outliers\n",
    "df5['promo_time_week'] = mms.fit_transform( df5[['promo_time_week']].values )\n",
    "pickle.dump(mms, open('parameter/promo_time_week_scaler.pkl', 'wb'))\n",
    "\n",
    "#--- year ---#\n",
    "# nenhum outlier\n",
    "df5['year'] = mms.fit_transform( df5[['year']].values )\n",
    "pickle.dump(mms, open('parameter/year_scaler.pkl', 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.254688Z",
     "start_time": "2021-02-08T19:24:35.534Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Visualizing Robust Scaling\n",
    "\n",
    "plt.figure(figsize=(20,7))\n",
    "plt.subplot(1,2,1)\n",
    "sns.distplot( df5['competition_distance'] ).set_title('antes do robust scaling')\n",
    "plt.subplot(1,2,2)\n",
    "sns.distplot( df5['competition_distance2'] ).set_title('depois do robust scaling')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 5.3. Transformação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 5.3.1. Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.259631Z",
     "start_time": "2021-02-08T19:24:35.541Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df5.select_dtypes(include='object')\n",
    "df5.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.261497Z",
     "start_time": "2021-02-08T19:24:35.545Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#------ state_holiday ------#\n",
    "# One hot encoding - utilizado para estados, é feriado ou não é feriado \n",
    "df5 = pd.get_dummies(df5, prefix=['state_holiday'], columns=['state_holiday'])\n",
    "\n",
    "#----- store_type -----#\n",
    "# Label encoder - utilizado porque aparentemente não há nenhuma relação de ordem entre as categorias da variável\n",
    "le = LabelEncoder()\n",
    "df5['store_type'] = le.fit_transform( df5['store_type'] )\n",
    "pickle.dump(le, open('parameter/store_type_scaler.pkl', 'wb'))\n",
    "\n",
    "#----- assortment -----#\n",
    "# Ordinal encoder - porque há uma relação de ordem entre as categorias basic < extra < extended\n",
    "assortment_dic = {'basic': 1, 'extra': 2, 'extended': 3}\n",
    "df5['assortment'] = df5['assortment'].map( assortment_dic )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.263155Z",
     "start_time": "2021-02-08T19:24:35.550Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df5.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 5.3.2. Transformação da variável resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.265007Z",
     "start_time": "2021-02-08T19:24:35.555Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#sns.distplot(df5['sales'])\n",
    "df5['sales'] = np.log1p( df5['sales'] )\n",
    "plt.figure(figsize=(15,7))\n",
    "sns.distplot(df5['sales'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 5.3.3. Transformação de Natureza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.266718Z",
     "start_time": "2021-02-08T19:24:35.560Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Variáveis cíclicas\n",
    "\n",
    "# day_of_week\n",
    "quant_days_of_week = 7\n",
    "df5['day_of_week_sin'] = df5['day_of_week'].apply(lambda x: np.sin( (x * 2. * np.pi) / quant_days_of_week ))\n",
    "df5['day_of_week_cos'] = df5['day_of_week'].apply(lambda x: np.cos( (x * 2. * np.pi) / quant_days_of_week ))\n",
    "\n",
    "# month\n",
    "quant_months = 12\n",
    "df5['month_sin'] = df5['month'].apply(lambda x: np.sin( (x * 2. * np.pi) / quant_months ))\n",
    "df5['month_cos'] = df5['month'].apply(lambda x: np.cos( (x * 2. * np.pi) / quant_months ))\n",
    "\n",
    "# day\n",
    "quant_days = 30\n",
    "df5['day_sin'] = df5['day'].apply(lambda x: np.sin( (x * 2. * np.pi) / quant_days ))\n",
    "df5['day_cos'] = df5['day'].apply(lambda x: np.cos( (x * 2. * np.pi) / quant_days ))\n",
    "\n",
    "# week_of_year\n",
    "quant_weeks_of_year = 52\n",
    "df5['day_sin'] = df5['week_of_year'].apply(lambda x: np.sin( (x * 2. * np.pi) / quant_weeks_of_year ))\n",
    "df5['day_cos'] = df5['week_of_year'].apply(lambda x: np.cos( (x * 2. * np.pi) / quant_weeks_of_year ))\n",
    "\n",
    "df5.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 6.0. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.268358Z",
     "start_time": "2021-02-08T19:24:35.565Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df6 = df5.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 6.1. Splitting dataframe into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.269946Z",
     "start_time": "2021-02-08T19:24:35.571Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.271544Z",
     "start_time": "2021-02-08T19:24:35.574Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cols_drop = ['week_of_year', 'day', 'month', 'day_of_week', 'promo_since', 'competition_since', 'year_week']\n",
    "df6 = df6.drop( cols_drop, axis = 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Como nós queremos prever 6 semanas de venda futura, vamos separar as últimas 6 semanas de venda para o test set, e todos os demais dados que constam vendas anteriores a esse período serão utilizados para treino do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.273121Z",
     "start_time": "2021-02-08T19:24:35.579Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 2015-07-10 00:00:00 - essa é a última data de venda que contempla todas as lojas\n",
    "aux = min(df6[['store','date']].groupby('store').max().reset_index()['date']) - datetime.timedelta( days = 6*7 )\n",
    "print(aux)\n",
    "\n",
    "# Dados de treino\n",
    "# training_set - datas antes de 2015-05-29 00:00:00\n",
    "x_train = df6[ df6['date'] < '2015-05-29']\n",
    "y_train = x_train['sales']\n",
    "\n",
    "# Test set\n",
    "# testar limitar a data para no máximo 6 semanas \n",
    "x_test = df6[ (df6['date'] >= '2015-05-29') & (df6['date'] < (aux+datetime.timedelta(days=6*7)) ) ]\n",
    "#x_test = df6[ df6['date'] >= '2015-05-29']\n",
    "y_test = x_test['sales']\n",
    "\n",
    "print('\\nTraining Min Date: ' + str( x_train['date'].min() ))\n",
    "print('Training Max Date: ' + str( x_train['date'].max() ))\n",
    "\n",
    "print('\\nTest Min Date: ' + str( x_test['date'].min() ))\n",
    "print('Test Max Date: ' + str( x_test['date'].max() ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 6.2. Boruta as Feature Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.274695Z",
     "start_time": "2021-02-08T19:24:35.586Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# training and test dataset for boruta\n",
    "x_train_vec = x_train.drop( ['date', 'sales'], axis=1 ).values\n",
    "y_train_vec = y_train.values.ravel()\n",
    "\n",
    "# n_jobs = -1 - Cria as árvores em paralelo\n",
    "#rf = RandomForestRegressor(n_jobs = -1)\n",
    "\n",
    "#boruta = BorutaPy( rf, n_estimators='auto', verbose=2, random_state=42).fit(x_train_vec, y_train_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 6.2.1. Best Features from Boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.276380Z",
     "start_time": "2021-02-08T19:24:35.590Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#cols_selected = boruta.support_.tolist()\n",
    "\n",
    "# best_features\n",
    "#x_train_fs = x_train.drop( ['date', 'sales'], axis=1 )\n",
    "#cols_selected_boruta = x_train_fs.iloc[:, cols_selected].columns.to_list()\n",
    "\n",
    "# not selected boruta\n",
    "#cols_not_selected_boruta = list(np.setdiff1d( x_train_fs.columns, cols_selected_boruta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 6.3. Manual Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.278477Z",
     "start_time": "2021-02-08T19:24:35.595Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# adicionamos o month_sin na lista de features relevantes\n",
    "\n",
    "cols_selected_boruta = ['store',\n",
    " 'promo',\n",
    " 'store_type',\n",
    " 'assortment',\n",
    " 'competition_distance',\n",
    " 'competition_open_since_month',\n",
    " 'competition_open_since_year',\n",
    " 'promo2',\n",
    " 'promo2_since_week',\n",
    " 'promo2_since_year',\n",
    " 'competition_time_month',\n",
    " 'promo_time_week',\n",
    " 'competition_distance2',\n",
    " 'day_of_week_sin',\n",
    " 'day_of_week_cos',\n",
    " 'month_sin',\n",
    " 'month_cos',\n",
    " 'day_sin',\n",
    " 'day_cos']\n",
    "\n",
    "cols_not_selected_boruta = ['is_promo2',\n",
    " 'month_sin',\n",
    " 'school_holiday',\n",
    " 'state_holiday_christimas',\n",
    " 'state_holiday_easter_holiday',\n",
    " 'state_holiday_public_holiday',\n",
    " 'state_holiday_regular_day',\n",
    " 'year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.280432Z",
     "start_time": "2021-02-08T19:24:35.599Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cols_selected_boruta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T18:36:52.327407Z",
     "start_time": "2021-01-30T18:35:15.242Z"
    }
   },
   "source": [
    "# 7.0. Machine Learning Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:47:28.026283Z",
     "start_time": "2021-02-08T19:47:28.005096Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train_aft_feat_select = x_train[ cols_selected_boruta ]\n",
    "x_test_aft_feat_select = x_test[ cols_selected_boruta ]\n",
    "\n",
    "# columns to add\n",
    "feat_to_add = ['date', 'sales']\n",
    "\n",
    "# final features\n",
    "cols_selected_boruta.extend( feat_to_add )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 7.1. Average Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.285177Z",
     "start_time": "2021-02-08T19:24:35.609Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# treino\n",
    "aux1 = x_train_aft_feat_select.copy()\n",
    "aux1['sales'] = y_train.copy()\n",
    "\n",
    "# teste\n",
    "aux3 = x_test_aft_feat_select.copy()\n",
    "aux3['sales'] = y_test.copy()\n",
    "\n",
    "# predictions\n",
    "aux2 = aux1[['store','sales']].groupby('store').mean().reset_index().rename( columns={'sales': 'predictions'} )\n",
    "aux3 = pd.merge( aux3, aux2, how='left', on='store')\n",
    "y_hat_baseline = aux3['predictions']\n",
    "\n",
    "# performance\n",
    "baseline_resul = ml_error('Average Model', np.expm1( y_test ), np.expm1( y_hat_baseline ))\n",
    "baseline_resul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 7.2. Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.286848Z",
     "start_time": "2021-02-08T19:24:35.616Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# model \n",
    "lr = LinearRegression()\n",
    "lr_fit = lr.fit( x_train_aft_feat_select, y_train )\n",
    "# prediction\n",
    "y_hat_lr = lr_fit.predict( x_test_aft_feat_select )\n",
    "\n",
    "# performance\n",
    "lr_resul = ml_error('Linear Regression', np.expm1( y_test ), np.expm1( y_hat_lr ) )\n",
    "lr_resul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 7.2.1. Linear Regression Model - Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.288476Z",
     "start_time": "2021-02-08T19:24:35.621Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lr_resul_cv = cross_val_err(x_train, 5, 'Linear Regression', lr)\n",
    "lr_resul_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 7.3. Linear Regression Regularized Model - Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.291196Z",
     "start_time": "2021-02-08T19:24:35.626Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# model \n",
    "## O fator alpha diminui o valor dos pesos, tornado-os menores\n",
    "lrr = Lasso( alpha=0.003 )\n",
    "\n",
    "lrr_fit = lrr.fit( x_train_aft_feat_select, y_train )\n",
    "# prediction\n",
    "y_hat_lrr = lrr_fit.predict( x_test_aft_feat_select )\n",
    "\n",
    "# performance\n",
    "lrr_resul = ml_error('Linear Regression Regularizes - Lasso', np.expm1( y_test ), np.expm1( y_hat_lrr ) )\n",
    "lrr_resul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 7.3.1. Linear Regression Regularized Model - Lasso - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.293526Z",
     "start_time": "2021-02-08T19:24:35.631Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lrr_resul_cv = cross_val_err(x_train, 5, 'Linear Regression Regularized - Lasso', lrr, True)\n",
    "lrr_resul_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 7.4. Random Forest Regressor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.297029Z",
     "start_time": "2021-02-08T19:24:35.636Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# model \n",
    "### rf = RandomForestRegressor( n_estimators=100, n_jobs=-1, random_state=42 )\n",
    "\n",
    "### rf_fit = rf_fit( x_train_aft_feat_select, y_train )\n",
    "\n",
    "# prediction\n",
    "### y_hat_rf = rf_fit.predict( x_test_aft_feat_select )\n",
    "\n",
    "# performance\n",
    "### rf_resul = ml_error('Random Forest', np.expm1( y_test ), np.expm1( y_hat_rf ) )\n",
    "rf_resul = pd.DataFrame( { 'Model Name': 'Random Forest',\n",
    "                'MAE': 720.685624,\n",
    "                'MAPE': 0.103639,\n",
    "                'RMSE': 1056.966469 }, index=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 7.4.1. Random Forest Regressor Model - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.298579Z",
     "start_time": "2021-02-08T19:24:35.641Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# rf_resul_cv = cross_val_err(x_train, 5, 'Random Forest Model', rf, True)\n",
    "# rf_resul_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 7.5. XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.300126Z",
     "start_time": "2021-02-08T19:24:35.646Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# model \n",
    "### model_xgb = xgb.XGBRegressor( objective='reg:squarederror',\n",
    "#                              n_estimators=100,\n",
    "#                              eta=0.01,\n",
    "#                              max_depth=10)\n",
    "\n",
    "# model_xgb_fit = model_xgb.fit( x_train_aft_feat_select, y_train )\n",
    "\n",
    "\n",
    "# prediction\n",
    "### y_hat_xgb = model_xgb_fit.predict( x_test_aft_feat_select )\n",
    "\n",
    "# performance\n",
    "### xgb_resul = ml_error('XGBoost Regressor', np.expm1( y_test ), np.expm1( y_hat_xgb ) )\n",
    "xgb_resul = pd.DataFrame( { 'Model Name': 'XGBoost Regressor',\n",
    "                'MAE': 6848.445061,\n",
    "                'MAPE': 0.951092,\n",
    "                'RMSE': 7526.587804 }, index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.301765Z",
     "start_time": "2021-02-08T19:24:35.651Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "xgb_resul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 7.5.1. XGBoost Regressor - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.303392Z",
     "start_time": "2021-02-08T19:24:35.655Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# xgb_resul_cv = cross_val_err(x_train, 5, 'XGBoost Regressor - Cross Validation', xgb_model, True)\n",
    "# xgb_resul_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 7.6. Comparing Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.305156Z",
     "start_time": "2021-02-08T19:24:35.660Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "models_performance = pd.concat( [baseline_resul, lr_resul, lrr_resul, rf_resul, xgb_resul] )\n",
    "models_performance.sort_values( 'RMSE' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Dado os erros é perceptível que o modelo Random Forest apresenta o menor erro de RMSE, porém vale lembrar que poucas semanas foram avaliadas no test set, portanto para ter uma performance mais acurada do modelo é necessário realizar o cross validation para testar o desempenho do modelo para diferentes cenários (pode ser que o teste feito agora era o melhor caso de certo modelo testado)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T20:53:38.911413Z",
     "start_time": "2021-02-01T20:53:37.603964Z"
    },
    "hidden": true
   },
   "source": [
    "<b>XGBoost utiliza menos memória do que a Random Forest</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 7.6.1. Comparing Model Performance - Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.309590Z",
     "start_time": "2021-02-08T19:24:35.668Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "models_performance_cv = pd.concat( [lr_resul_cv, lrr_resul_cv, rf_resul_cv, xgb_resul_cv] )\n",
    "models_performance_cv.sort_values( 'RMSE' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.0. Hyperparameter Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 8.1. Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:56:35.262929Z",
     "start_time": "2021-02-08T19:56:35.257794Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "param = {\n",
    "            'n_estimators': [100, 500, 1000],\n",
    "            'max_depth': [10, 20, 50]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:56:36.391525Z",
     "start_time": "2021-02-08T19:56:35.780769Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "MAX_ITER = 5\n",
    "final_result = pd.DataFrame()\n",
    "\n",
    "for i in range( MAX_ITER ):\n",
    "    # choose values for parameters randomly\n",
    "    hft = { k: random.sample( v, 1 )[0] for k, v in param.items() }\n",
    "    print( hft )\n",
    "\n",
    "    # model \n",
    "    rf = RandomForestRegressor( n_estimators=hft['n_estimators'],\n",
    "                                max_depth=hft['max_depth'],\n",
    "                                n_jobs=-1,\n",
    "                                random_state=42 )\n",
    "\n",
    "    # performance\n",
    "    rf_resul_cv = cross_val_err(x_train, 5, 'Random Forest Model', rf, True)\n",
    "    final_result = pd.concat( [final_result, rf_resul_cv ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:43:37.381435Z",
     "start_time": "2021-02-08T19:43:37.374363Z"
    }
   },
   "source": [
    "## 8.2. Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_tuned = {\n",
    "                'n_estimators': ,\n",
    "                'max_depth': \n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model \n",
    "rf_tuned = RandomForestRegressor( n_estimators= param_tuned['n_estimators'],\n",
    "                            max_depth= param_tuned['max_depth'],\n",
    "                            n_jobs=-1,\n",
    "                            random_state=42 ).fit( x_train_aft_feat_select, y_train )\n",
    "\n",
    "# prediction\n",
    "y_hat_rf_tuned = rf_tuned.predict( x_test_aft_feat_select )\n",
    "\n",
    "# performance\n",
    "rf_tuned_resul = ml_error('Random Forest', np.expm1( y_test ), np.expm1( y_hat_rf_tuned ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 9.0. Error translation and interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.311654Z",
     "start_time": "2021-02-08T19:24:35.674Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df9 = x_test[ cols_selected_boruta ]\n",
    "\n",
    "df9['sales'] = np.expm1( df9['sales'] )\n",
    "df9['predictions'] = np.expm1( y_hat_lrr )\n",
    "# irá conter a quantidade de dados de cada loja\n",
    "df9['quant'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T16:26:34.088006Z",
     "start_time": "2021-02-03T16:26:34.043404Z"
    },
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 9.1. Business Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.313632Z",
     "start_time": "2021-02-08T19:24:35.679Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "aux = df9[['store','quant']].groupby('store').count()\n",
    "df90 = df9[['store', 'sales']].groupby('store').sum().reset_index()\n",
    "df91 = df9[['store', 'predictions']].groupby('store').sum().reset_index()\n",
    "\n",
    "# MAE e MAPE\n",
    "df92 = df9[['store', 'sales', 'predictions']].groupby('store').apply( lambda x: mean_absolute_error(x['sales'], x['predictions']) ).reset_index().rename( columns={ 0: 'MAE' })\n",
    "df93 = df9[['store', 'sales', 'predictions']].groupby('store').apply( lambda x: mean_absolute_percentage_error(x['sales'], x['predictions']) ).reset_index().rename( columns={ 0: 'MAPE' })\n",
    "\n",
    "# Merge\n",
    "df94 = pd.merge(df92, df93, how='inner', on='store')\n",
    "df95 = pd.merge(df91, df94, how='inner', on='store')\n",
    "df96 = pd.merge(df90, df95, how='inner', on='store')\n",
    "df97 = pd.merge(aux, df96, how='inner', on='store')\n",
    "\n",
    "# Scenarios\n",
    "# Como as vendas de 6 semanas foram somadas, é necessário que o MAE seja multiplicado pela quantidade de vendas\n",
    "# para assim indicar corretamente o melhor e pior cenário\n",
    "df97['worst_scenario'] = df97['predictions'] - (df97['MAE'] * df97['quant'])\n",
    "df97['best_scenario'] = df97['predictions'] + (df97['MAE'] * df97['quant'])\n",
    "\n",
    "# order columns\n",
    "df97 = df97[['store', 'sales', 'predictions', 'worst_scenario', 'best_scenario', 'MAE', 'MAPE']]\n",
    "df97"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Piores MAPEs - Mostra que há lojas mais difíceis de prever "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.315405Z",
     "start_time": "2021-02-08T19:24:35.685Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df97.sort_values('MAPE', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### MAPEs de todas as lojas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.317312Z",
     "start_time": "2021-02-08T19:24:35.689Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "sns.scatterplot( x='store', y='MAPE', data=df97)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 9.2. Total Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.319091Z",
     "start_time": "2021-02-08T19:24:35.694Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df98 = df97[['predictions', 'best_scenario', 'worst_scenario']].apply( lambda x: np.sum( x ), axis=0).reset_index().rename( columns={'Index':'Scenario', 0: 'Values'} )\n",
    "df98['Values'] = df98['Values'].map('R$ {:,.2f}'.format)\n",
    "df98"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 9.3. Machine Learning Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T20:32:27.104257Z",
     "start_time": "2021-02-03T20:32:27.098180Z"
    },
    "hidden": true
   },
   "source": [
    "<b> A sombra no gráfico representa o erro </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T20:37:31.007821Z",
     "start_time": "2021-02-03T20:37:31.003007Z"
    },
    "hidden": true
   },
   "source": [
    "<b> Dica: Para entender melhor o erro - Análise de Resíduo </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.320745Z",
     "start_time": "2021-02-08T19:24:35.702Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df9['error'] = df9['sales'] - df9['predictions']\n",
    "df9['error_rate'] = df9['predictions'] / df9['sales']\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "sns.lineplot(x='date', y='sales', data=df9, label='sales')\n",
    "sns.lineplot(x='date', y='predictions', data=df9, label='predictions')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "sns.lineplot(x='date', y='error_rate', data=df9, label='error rate')\n",
    "plt.axhline(1, linestyle='--')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "sns.distplot( df9['error'] )\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "sns.scatterplot( df9['predictions'], df9['error'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "gráfico 3 - ser próximo de uma normal\n",
    "\n",
    "gráfico 4 - o erro precisa estar dentro de um 'tubo'\n",
    "\n",
    "Se seguir tais critérios, o modelo é considerado bom para a análise de resíduos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.0. Modelo em produção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.322268Z",
     "start_time": "2021-02-08T19:24:35.708Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "pickle.dump( lrr_fit, open('/home/gabriel/Desktop/dataScience-ML-AI-DL/data-science/sejaUmDataScientist/rossMann/model/model_rossmann.pkl', 'wb') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1. Rossmann Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.326089Z",
     "start_time": "2021-02-08T19:24:35.713Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import inflection\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "class Rossmann( object ):\n",
    "    \n",
    "    def __init__( self ):\n",
    "        self.home_path = '/home/gabriel/Desktop/dataScience-ML-AI-DL/data-science/sejaUmDataScientist/rossMann/'\n",
    "        self.competition_distance2_scaler = pickle.load( open( self.home_path + 'parameter/competition_distance2_scaler.pkl', 'rb') )\n",
    "        self.competition_time_month_scaler = pickle.load( open( self.home_path + 'parameter/competition_time_month_scaler.pkl', 'rb') )\n",
    "        self.promo_time_week_scaler = pickle.load( open( self.home_path + 'parameter/promo_time_week_scaler.pkl', 'rb') )\n",
    "        self.year_scaler = pickle.load( open( self.home_path + 'parameter/year_scaler.pkl', 'rb') )\n",
    "        self.store_type_scaler = pickle.load( open( self.home_path + 'parameter/store_type_scaler.pkl', 'rb') )\n",
    "\n",
    "    \n",
    "    def data_cleaning( self, df1 ):\n",
    "\n",
    "        ## 1.1. Rename Columns\n",
    "\n",
    "        cols_old = ['Store', 'DayOfWeek', 'Date', 'Open', 'Promo',\n",
    "                    'StateHoliday', 'SchoolHoliday', 'StoreType', 'Assortment',\n",
    "                    'CompetitionDistance', 'CompetitionOpenSinceMonth',\n",
    "                    'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek',\n",
    "                    'Promo2SinceYear', 'PromoInterval']\n",
    "        \n",
    "        snakecase = lambda x : inflection.underscore(x)\n",
    "\n",
    "        # map applies a lambda function to something\n",
    "        cols_new = list( map( snakecase, cols_old) )\n",
    "\n",
    "        # rename\n",
    "        df1.columns = cols_new\n",
    "\n",
    "        ## 1.3. Data Types \n",
    "\n",
    "        # Converting date from object to date\n",
    "        df1['date'] = pd.to_datetime( df1['date'] )\n",
    "\n",
    "\n",
    "        ## 1.5. Fillout NA\n",
    "\n",
    "        ### competition_distance\n",
    "\n",
    "        competition_distance_max = df1['competition_distance'].max()\n",
    "\n",
    "        # competition_distance - the NA will be replaced by 2000 * competition_distance_max\n",
    "        df1['competition_distance'] = df1['competition_distance'].apply(lambda x : (4 * competition_distance_max) if math.isnan(x) else x)\n",
    "\n",
    "        # competition_open_since_month\n",
    "        df1['competition_open_since_month'] = df1.apply(lambda x : x['date'].month if math.isnan( x['competition_open_since_month'] ) else x['competition_open_since_month'], axis = 1)\n",
    "\n",
    "        # competition_open_since_year\n",
    "        df1['competition_open_since_year'] = df1.apply(lambda x : x['date'].year if math.isnan( x['competition_open_since_year'] ) else x['competition_open_since_year'], axis = 1)\n",
    "\n",
    "        # promo2_since_week\n",
    "        df1['promo2_since_week'] = df1.apply(lambda x : x['date'].week if math.isnan(x['promo2_since_week']) else x['promo2_since_week'], axis=1)\n",
    "\n",
    "        # promo2_since_year\n",
    "        df1['promo2_since_year'] = df1.apply(lambda x : x['date'].year if math.isnan(x['promo2_since_year']) else x['promo2_since_year'], axis=1)\n",
    "\n",
    "        # promo_interval   \n",
    "        month_map = { 1: 'Jan', 2: 'Feb', 3: 'Mar', 4: 'Apr', 5: 'May', 6: 'Jun', 7: 'Jul', \n",
    "                    8: 'Aug', 9: 'Sep', 10: 'Out', 11: 'Nov', 12: 'Dec'}\n",
    "\n",
    "        # inplace=True do the modification directly in the object instead of returning the modification\n",
    "        df1['promo_interval'].fillna(0, inplace=True)\n",
    "\n",
    "        df1['month_map'] = df1['date'].dt.month.map( month_map )\n",
    "\n",
    "        df1['is_promo2'] = df1[['promo_interval', 'month_map']].apply(lambda x : 0 if x['promo_interval'] == 0 else 1 if x['month_map'] in x['promo_interval'].split(',') else 0, axis=1)  \n",
    "\n",
    "\n",
    "        ## 1.6 Change Types\n",
    "\n",
    "        df1['competition_open_since_month'] = df1['competition_open_since_month'].astype(int)\n",
    "        df1['competition_open_since_year'] = df1['competition_open_since_year'].astype(int)\n",
    "        df1['promo2_since_week'] = df1['promo2_since_week'].astype(int)\n",
    "        df1['promo2_since_year'] = df1['promo2_since_year'].astype(int)\n",
    "        \n",
    "        return df1\n",
    "\n",
    "\n",
    "    def feature_engineering(self, df2):\n",
    "            \n",
    "        ## Derivando variáveis\n",
    "\n",
    "        # year\n",
    "        df2['year'] = df2['date'].dt.year\n",
    "\n",
    "        # month\n",
    "        df2['month'] = df2['date'].dt.month\n",
    "\n",
    "        # day\n",
    "        df2['day'] = df2['date'].dt.day\n",
    "\n",
    "        # week of year\n",
    "        df2['week_of_year'] = df2['date'].dt.week\n",
    "\n",
    "        # year week - changing the date format\n",
    "        df2['year_week'] = df2['date'].dt.strftime('%Y-%W')\n",
    "\n",
    "        # competition since\n",
    "        df2['competition_since'] = df2.apply(lambda x: datetime.datetime(year=x['competition_open_since_year'] , month=x['competition_open_since_month'], day=1), axis=1)\n",
    "        df2['competition_time_month'] = ((df2['date'] - df2['competition_since']) / 30).apply(lambda x: x.days).astype(int)\n",
    "\n",
    "        # promo since\n",
    "        df2['promo_since'] = df2['promo2_since_year'].astype(str) + '-' + df2['promo2_since_week'].astype(str)\n",
    "        df2['promo_since'] = df2['promo_since'].apply(lambda x: datetime.datetime.strptime(x + '-1', '%Y-%W-%w') - datetime.timedelta(days=7))\n",
    "        df2['promo_time_week'] = ((df2['date'] - df2['promo_since'])/7).apply(lambda x: x.days).astype(int)\n",
    "\n",
    "        # assortment\n",
    "        # level: a = basic, b = extra, c = extended\n",
    "        df2['assortment'] = df2['assortment'].apply(lambda x: 'basic' if x == 'a' else 'extra' if x == 'b' else 'extended')\n",
    "\n",
    "        # state holiday\n",
    "        # a = public holiday, b = Easter holiday, c = Christmas, 0 = None\n",
    "        df2['state_holiday'] = df2['state_holiday'].apply(lambda x: 'public_holiday' if x == 'a' else 'easter_holiday' if x == 'b' else 'christimas' if x == 'c' else 'regular_day')\n",
    "\n",
    "\n",
    "        # 3.0. Filtragem de variáveis\n",
    "\n",
    "        ## 3.2. Seleção das colunas\n",
    "\n",
    "        cols_drop = ['open', 'promo_interval', 'month_map']\n",
    "        df2 = df2.drop(cols_drop, axis = 1)\n",
    "        \n",
    "        return df2\n",
    "\n",
    "    def data_preparation( self, df5 ):\n",
    "\n",
    "        ## 5.2. Rescaling\n",
    "\n",
    "        aux = df5.select_dtypes(include = ['int64', 'float64'])\n",
    "\n",
    "        #--- competition_distance ---#\n",
    "        # Encontra os parâmetros (quartiles) e aplica na variável\n",
    "        df5['competition_distance2'] = self.competition_distance2_scaler.fit_transform( df5[['competition_distance']].values )\n",
    "\n",
    "        #--- competition time month ---#\n",
    "        df5['competition_time_month'] = self.competition_time_month_scaler.fit_transform( df5[['competition_time_month']].values )\n",
    "\n",
    "        #--- promo_time_week ---#\n",
    "        # poucos outliers\n",
    "        df5['promo_time_week'] = self.promo_time_week_scaler.fit_transform( df5[['promo_time_week']].values )\n",
    "\n",
    "        #--- year ---#\n",
    "        # nenhum outlier\n",
    "        df5['year'] = self.year_scaler.fit_transform( df5[['year']].values )\n",
    "\n",
    "        ## 5.3. Transformação\n",
    "\n",
    "        ### 5.3.1. Encoding\n",
    "\n",
    "        #------ state_holiday ------#\n",
    "        # One hot encoding - utilizado para estados, é feriado ou não é feriado \n",
    "        df5 = pd.get_dummies(df5, prefix=['state_holiday'], columns=['state_holiday'])\n",
    "\n",
    "        #----- store_type -----#\n",
    "        # Label encoder - utilizado porque aparentemente não há nenhuma relação de ordem entre as categorias da variável\n",
    "        df5['store_type'] = self.store_type_scaler.fit_transform( df5['store_type'] )\n",
    "\n",
    "        #----- assortment -----#\n",
    "        # Ordinal encoder - porque há uma relação de ordem entre as categorias basic < extra < extended\n",
    "        assortment_dic = {'basic': 1, 'extra': 2, 'extended': 3}\n",
    "        df5['assortment'] = df5['assortment'].map( assortment_dic )\n",
    "\n",
    "\n",
    "        ### 5.3.3. Transformação de Natureza\n",
    "\n",
    "        # Variáveis cíclicas\n",
    "\n",
    "        # day_of_week\n",
    "        quant_days_of_week = 7\n",
    "        df5['day_of_week_sin'] = df5['day_of_week'].apply(lambda x: np.sin( (x * 2. * np.pi) / quant_days_of_week ))\n",
    "        df5['day_of_week_cos'] = df5['day_of_week'].apply(lambda x: np.cos( (x * 2. * np.pi) / quant_days_of_week ))\n",
    "\n",
    "        # month\n",
    "        quant_months = 12\n",
    "        df5['month_sin'] = df5['month'].apply(lambda x: np.sin( (x * 2. * np.pi) / quant_months ))\n",
    "        df5['month_cos'] = df5['month'].apply(lambda x: np.cos( (x * 2. * np.pi) / quant_months ))\n",
    "\n",
    "        # day\n",
    "        quant_days = 30\n",
    "        df5['day_sin'] = df5['day'].apply(lambda x: np.sin( (x * 2. * np.pi) / quant_days ))\n",
    "        df5['day_cos'] = df5['day'].apply(lambda x: np.cos( (x * 2. * np.pi) / quant_days ))\n",
    "\n",
    "        # week_of_year\n",
    "        quant_weeks_of_year = 52\n",
    "        df5['day_sin'] = df5['week_of_year'].apply(lambda x: np.sin( (x * 2. * np.pi) / quant_weeks_of_year ))\n",
    "        df5['day_cos'] = df5['week_of_year'].apply(lambda x: np.cos( (x * 2. * np.pi) / quant_weeks_of_year ))\n",
    "\n",
    "        cols_selected = [ 'store', 'promo', 'store_type', 'assortment', 'competition_distance',\n",
    "                                 'competition_open_since_month', 'competition_open_since_year', 'promo2',\n",
    "                                 'promo2_since_week', 'promo2_since_year', 'competition_time_month',\n",
    "                                 'promo_time_week', 'competition_distance2', 'day_of_week_sin',\n",
    "                                 'day_of_week_cos', 'month_sin', 'month_cos', 'day_sin', 'day_cos']\n",
    "\n",
    "        return df5[ cols_selected ]\n",
    "    \n",
    "    \n",
    "    def get_prediction( self, model, original_data, test_data ):\n",
    "        # prediction\n",
    "        pred = model.predict( test_data )\n",
    "        \n",
    "        # join pred into the original data\n",
    "        original_data['prediction'] = np.expm1( pred ) \n",
    "        \n",
    "        return original_data.to_json( orient='records', date_format='iso' )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2. API Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.330322Z",
     "start_time": "2021-02-08T19:24:35.720Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from flask import Flask, request, Response\n",
    "from rossmann.Rossmann import Rossmann\n",
    "\n",
    "\n",
    "# loading model\n",
    "model = pickle.load( open('/home/gabriel/Desktop/dataScience-ML-AI-DL/data-science/sejaUmDataScientist/rossMann/model/model_rossmann.pkl', 'rb') )\n",
    "\n",
    "# initialize API\n",
    "app = Flask( __name__ )\n",
    "\n",
    "@app.route( '/rossmann/predict', methods=['POST'] )\n",
    "\n",
    "def rossmann_predict():\n",
    "    test_json = request.get_json()\n",
    "    \n",
    "    # there is data\n",
    "    if test_json:\n",
    "        \n",
    "        # unique data\n",
    "        if isinstance( test_json, dict ):\n",
    "            test_raw = pd.DataFrame( test_json, index=[0] )\n",
    "        \n",
    "        # multiple data\n",
    "        else:\n",
    "            test_raw = pd.DataFrame( test_json, columns=test_json[0].keys() )\n",
    "            \n",
    "        # Instantiate Rossmann class\n",
    "        pipeline = Rossmann()\n",
    "        \n",
    "        # data cleaning\n",
    "        df1 = pipeline.data_cleaning( test_raw )\n",
    "        \n",
    "        # feature engineering\n",
    "        df2 = pipeline.feature_engineering( df1 )\n",
    "        \n",
    "        # data preparation\n",
    "        df3 = pipeline.data_preparation( df2 ) \n",
    "        \n",
    "        # prediction\n",
    "        df_response = pipeline.get_prediction( model, test_raw, df3 )\n",
    "        \n",
    "        return df_response\n",
    "        \n",
    "    # there is no data    \n",
    "    else:\n",
    "        return Response( '{}', status=200, mimetype='application/json' )\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run( '0.0.0.0' )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3. API Tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.332073Z",
     "start_time": "2021-02-08T19:24:35.725Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.333683Z",
     "start_time": "2021-02-08T19:24:35.728Z"
    }
   },
   "outputs": [],
   "source": [
    "# loading test dataset\n",
    "df10 = pd.read_csv('/home/gabriel/Desktop/dataScience-ML-AI-DL/data-science/sejaUmDataScientist/rossMann/data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.335314Z",
     "start_time": "2021-02-08T19:24:35.732Z"
    }
   },
   "outputs": [],
   "source": [
    "# merge test dataset + store\n",
    "df_test = pd.merge( df10, df_store_raw, how='left', on='Store' )\n",
    "\n",
    "# choose store for prediction\n",
    "df_test = df_test[ df_test['Store'].isin( [15,22] ) ]\n",
    "\n",
    "# remove closed days\n",
    "df_test = df_test[ df_test['Open'] != 0 ]\n",
    "df_test = df_test[ ~df_test['Open'].isnull() ]\n",
    "df_test = df_test.drop( 'Id', axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.337002Z",
     "start_time": "2021-02-08T19:24:35.736Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert dataframe to json\n",
    "data = json.dumps( df_test.to_dict( orient='records' ) )\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.338535Z",
     "start_time": "2021-02-08T19:24:35.739Z"
    }
   },
   "outputs": [],
   "source": [
    "# API Call\n",
    "# url = 'http://0.0.0.0:5000/rossmann/predict'\n",
    "url = 'https://rossmann-model-main.herokuapp.com/rossmann/predict'\n",
    "header = {'Content-type': 'application/json'}\n",
    "data = data\n",
    "\n",
    "response = requests.post( url, data=data, headers=header )\n",
    "print('Status code: ' + str( response.status_code ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.343298Z",
     "start_time": "2021-02-08T19:24:35.743Z"
    }
   },
   "outputs": [],
   "source": [
    "df_resp = pd.DataFrame( response.json(), columns=response.json()[0].keys() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.347398Z",
     "start_time": "2021-02-08T19:24:35.747Z"
    }
   },
   "outputs": [],
   "source": [
    "df_resp = df_resp[['store', 'prediction']].groupby('store').sum().reset_index()\n",
    "\n",
    "for i in range( len(df_resp) ):\n",
    "    print('Store number {} will sell R$ {:,.2f} in the next 6 weeks'.format(\n",
    "        df_resp.loc[i, 'store'],\n",
    "        df_resp.loc[i, 'prediction'] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:27:29.349442Z",
     "start_time": "2021-02-08T19:24:35.752Z"
    }
   },
   "outputs": [],
   "source": [
    "df_resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
